{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS MODULE\n",
    "\n",
    "import operator\n",
    "\n",
    "\n",
    "class RefPath:\n",
    "    INITIAL = \"#/info/x-clusters/\"\n",
    "    WORKER_NODES = \"worker-nodes\"\n",
    "    PODS = \"pods\"\n",
    "    CONTAINERS = \"containers\"\n",
    "    INFO = \"info\"\n",
    "    X_CLUSTERS = \"x-clusters\"\n",
    "    X_LOCATION = \"x-location\"\n",
    "    REF = \"$ref\"\n",
    "    PATHS = \"paths\"\n",
    "\n",
    "    def __init__(self, *arg):\n",
    "        if len(arg) == 1:\n",
    "            paths = arg[0].split(\"#/info/x-clusters/\")\n",
    "            parts = paths[1].split(\"/\")\n",
    "            self.cluster = parts[0]\n",
    "            self.worker_node = parts[2]\n",
    "            self.pod_name = parts[4]\n",
    "            self.container_name = parts[6]\n",
    "        else:\n",
    "            self.cluster = arg[0]\n",
    "            self.worker_node = arg[1]\n",
    "            self.pod_name = arg[2]\n",
    "            self.container_name = arg[3]\n",
    "\n",
    "    @property\n",
    "    def full_path(self):\n",
    "        return f\"{RefPath.INITIAL}{self.cluster}/{RefPath.WORKER_NODES}/{self.worker_node}/{RefPath.PODS}/\" \\\n",
    "               f\"{self.pod_name}/{RefPath.CONTAINERS}/{self.container_name}\"\n",
    "\n",
    "\n",
    "class Method:\n",
    "    def __init__(self, path_name, method_name, ref_path, load, schema_name, full_method):\n",
    "        self.path_name = path_name\n",
    "        self.method_name = method_name\n",
    "        self.ref_path = ref_path\n",
    "        self.load = load\n",
    "        self.schema_name = schema_name\n",
    "        self.full_method = full_method\n",
    "        self.contribution = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.path_name + \": \" + self.method_name + \" (\" + self.ref_path.full_path + \") \" + str(\n",
    "            self.load) + \" <-> \" + self.schema_name\n",
    "\n",
    "\n",
    "class Component:\n",
    "    def __init__(self, name, load, full_component, ref_path=None, is_new=False):\n",
    "        self.name = name\n",
    "        self.load = load\n",
    "        self.full_component = full_component\n",
    "        self.ref_path = ref_path\n",
    "        self.is_new = is_new\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name + \" ---- \" + str(self.load) + \" (\" + self.ref_path + \") \" + \"New ? \" + str(self.is_new)\n",
    "\n",
    "\n",
    "class Group:\n",
    "    def __init__(self, parent_load, sum_of_loads, components):\n",
    "        self.parent_load = parent_load\n",
    "        self.sum_of_loads = sum_of_loads\n",
    "        self.components = components\n",
    "\n",
    "    def get_contributed_components(self):\n",
    "        components = []\n",
    "        for component in self.components:\n",
    "            component.contribution = (component.load / self.sum_of_loads) * self.parent_load\n",
    "            components.append(component)\n",
    "        return sorted(components, key=operator.attrgetter(\"contribution\"))\n",
    "\n",
    "\n",
    "class Cluster(Component):\n",
    "    def __init__(self, name, load, full_component, ref_path, is_new=False):\n",
    "        super().__init__(name, load, full_component, ref_path, is_new)\n",
    "        self.is_cluster = True\n",
    "\n",
    "    @property\n",
    "    def worker_nodes(self):\n",
    "        worker_nodes = []\n",
    "        for wn in self.full_component[\"worker-nodes\"].values():\n",
    "            ref_path = self.ref_path + \"/worker-nodes/\" + wn[\"name\"]\n",
    "            worker_nodes.append(WorkerNode(wn[\"name\"], wn[\"metrics\"][\"load\"], wn, ref_path))\n",
    "        return worker_nodes\n",
    "\n",
    "\n",
    "class WorkerNode(Component):\n",
    "    def __init__(self, name, load, full_component, ref_path, is_new=False):\n",
    "        super().__init__(name, load, full_component, ref_path, is_new)\n",
    "        self.is_worker_node = True\n",
    "\n",
    "    @property\n",
    "    def pods(self):\n",
    "        pods = []\n",
    "        for pod in self.full_component[\"pods\"].values():\n",
    "            ref_path = self.ref_path + \"/pods/\" + pod[\"name\"]\n",
    "            pods.append(Pod(pod[\"name\"], pod[\"metrics\"][\"load\"], pod, ref_path))\n",
    "        return pods\n",
    "\n",
    "\n",
    "class Pod(Component):\n",
    "    def __init__(self, name, load, full_component, ref_path, is_new=False):\n",
    "        super().__init__(name, load, full_component, ref_path, is_new)\n",
    "        self.is_pod = True\n",
    "        self.contribution = 0\n",
    "\n",
    "    @property\n",
    "    def containers(self):\n",
    "        containers = []\n",
    "        for container_name, container in self.full_component[\"containers\"].items():\n",
    "            ref_path = self.ref_path + \"/containers/\" + container_name\n",
    "            containers.append(Container(container_name, container[\"metrics\"][\"load\"], container, ref_path))\n",
    "        return containers\n",
    "\n",
    "\n",
    "class Container(Component):\n",
    "    def __init__(self, name, load, full_component, ref_path, is_new=False):\n",
    "        super().__init__(name, load, full_component, ref_path, is_new)\n",
    "        self.is_container = True\n",
    "\n",
    "\n",
    "class PodGroup(Group):\n",
    "    def __init__(self, parent_load, sum_of_loads, components):\n",
    "        super().__init__(parent_load, sum_of_loads, components)\n",
    "        self.is_pod_group = True\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"PodGroup sum_of_loads: \" + str(self.sum_of_loads)\n",
    "\n",
    "\n",
    "class ContainerGroup(Group):\n",
    "    def __init__(self, parent_load, sum_of_loads, components):\n",
    "        super().__init__(parent_load, sum_of_loads, components)\n",
    "        self.is_container_group = True\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"ContainerGroup sum_of_loads: \" + str(self.sum_of_loads)\n",
    "\n",
    "\n",
    "class MethodGroup(Group):\n",
    "    def __init__(self, parent_load, sum_of_loads, components):\n",
    "        super().__init__(parent_load, sum_of_loads, components)\n",
    "        self.is_method_group = True\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"ContainerGroup sum_of_loads: \" + str(self.sum_of_loads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITIES MODULES DEFINED HERE\n",
    "\n",
    "def _gen_dict_extract(key, var):\n",
    "    if hasattr(var, 'items'):\n",
    "        for k, v in var.items():\n",
    "            if k == key:\n",
    "                yield v\n",
    "            if isinstance(v, dict):\n",
    "                for result in _gen_dict_extract(key, v):\n",
    "                    yield result\n",
    "            elif isinstance(v, (list)):\n",
    "                for d in v:\n",
    "                    for result in _gen_dict_extract(key, d):\n",
    "                        yield result\n",
    "\n",
    "\n",
    "def _get_schema_only(references):\n",
    "    saved_schema = 'default'\n",
    "    for reference in references:\n",
    "        schema = reference.split(\"#/components/schemas/\")\n",
    "        try:\n",
    "            saved_schema = schema[1]\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "    return saved_schema\n",
    "\n",
    "\n",
    "def _get_schemas_only(references):\n",
    "    saved_schemas = []\n",
    "    for reference in references:\n",
    "        schema = reference.split(\"#/components/schemas/\")\n",
    "        try:\n",
    "            saved_schemas.append(schema[1])\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "    return saved_schemas\n",
    "\n",
    "\n",
    "def _join_components(threshold, components):\n",
    "    joined_components = []\n",
    "    remaining_components = []\n",
    "    collective_contribution = 0\n",
    "    for i in range(len(components)):\n",
    "        component = components[i]\n",
    "        collective_contribution += component.contribution\n",
    "\n",
    "        if collective_contribution < threshold:\n",
    "            joined_components.append(component)\n",
    "        else:\n",
    "            remaining_components.append(component)\n",
    "\n",
    "    return joined_components, remaining_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS MODULES\n",
    "\n",
    "MAX_WN_LOAD = 50\n",
    "MIN_WN_LOAD = 20\n",
    "MAX_POD_LOAD = 50\n",
    "MIN_POD_LOAD = 30\n",
    "DEFAULT_SCHEMA_NAME = 'default'\n",
    "POD_LEVEL = \"pod\"\n",
    "CL_LEVEL = \"cluster\"\n",
    "WN_LEVEL = \"worker-node\"\n",
    "SCHEMA_LEVEL = \"x-storage-level\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"01data.json\") as f:    \n",
    "    dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _adjust_and_merge_pods(scalable_pods, methods, all_pods):\n",
    "    min_load_pods = []\n",
    "    \n",
    "#     if there is only one pod it will return as it is\n",
    "    if len(scalable_pods) < 2:\n",
    "        return scalable_pods\n",
    "\n",
    "#     creating a new list so that we can modify the original one\n",
    "    for pod in list(scalable_pods):\n",
    "        if pod.load <= MIN_POD_LOAD:\n",
    "            min_load_pods.append(pod)            \n",
    "            scalable_pods.remove(pod)\n",
    "            all_pods.pop(pod.name)\n",
    "    \n",
    "    first_pod = scalable_pods[0]    \n",
    "#     print(first_pod.full_component)\n",
    "    for pod in min_load_pods:\n",
    "        first_pod.full_component['metrics']['load'] += pod.load\n",
    "        \n",
    "        for container in pod.containers:            \n",
    "            \n",
    "            container_methods = filter(lambda method: method.ref_path.full_path == container.ref_path, methods)\n",
    "            first_pod_containers = first_pod.full_component[RefPath.CONTAINERS]\n",
    "            new_container_name = \"c\" + str((len(first_pod_containers) + 1))\n",
    "            first_pod_containers[new_container_name] = container.full_component\n",
    "                        \n",
    "#             don't forget to chage the id of container\n",
    "            first_pod_containers[new_container_name]['id'] = new_container_name\n",
    "    \n",
    "            for container_method in container_methods:\n",
    "                print(\"Changed from:\", container_method.ref_path.full_path)\n",
    "                \n",
    "#                 getting ref_path of first container in first selected pod\n",
    "                \n",
    "                container_method.ref_path = RefPath(first_pod.containers[0].ref_path)\n",
    "                container_method.ref_path.container_name = new_container_name\n",
    "                \n",
    "                print(\"To:\", container_method.ref_path.full_path)\n",
    "#         print(first_pod.full_component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _derive_components(single_data_obj):\n",
    "    clusters = []\n",
    "    cls = single_data_obj[RefPath.X_CLUSTERS]\n",
    "\n",
    "    for cl in cls.values():\n",
    "        ref_path = RefPath.INITIAL + cl[\"name\"]\n",
    "        clusters.append(Cluster(cl[\"name\"], cl[\"metrics\"][\"load\"], cl, ref_path))\n",
    "\n",
    "    methods = []\n",
    "    data_paths = single_data_obj[RefPath.PATHS]\n",
    "    for path_name, path in data_paths.items():\n",
    "        for method_name, method in path.items():\n",
    "            ref_path = RefPath(method[RefPath.X_LOCATION][RefPath.REF])\n",
    "\n",
    "            all_references = list(set(_gen_dict_extract('$ref', method)))\n",
    "            schema_name = _get_schema_only(all_references)\n",
    "\n",
    "            methods.append(\n",
    "                Method(path_name, method_name, ref_path, method[\"x-metrics\"][\"load\"], schema_name, method))\n",
    "    return clusters, methods\n",
    "\n",
    "def _get_scalable_components(clusters, methods):\n",
    "    scalable_wns = []\n",
    "    scalable_pods = []\n",
    "    for cluster in clusters:\n",
    "        \n",
    "        _adjust_and_merge_wns(cluster.worker_nodes, methods, cluster.full_component[RefPath.WORKER_NODES])\n",
    "        for worker_node in cluster.worker_nodes:\n",
    "            if MIN_WN_LOAD < worker_node.load < MAX_WN_LOAD:                \n",
    "                print(\"no need to scale \", str(worker_node))\n",
    "                \n",
    "#                 _adjust_and_merge_pods(worker_node.pods, methods, worker_node.full_component[RefPath.PODS])\n",
    "                for wn_pod in worker_node.pods:                    \n",
    "                    if MIN_POD_LOAD < wn_pod.load < MAX_POD_LOAD:\n",
    "                        print(\"no need to scale\", str(wn_pod))\n",
    "                    else:\n",
    "                        print(wn_pod, \" need scaling.\")\n",
    "                        scalable_pods.append(wn_pod)\n",
    "            else:\n",
    "                print(worker_node, \" need scaling.\")\n",
    "                scalable_wns.append(worker_node)\n",
    "    return scalable_wns, scalable_pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _adjust_and_merge_wns(scalable_wns, methods, all_nodes):\n",
    "    min_load_wns = []\n",
    "    \n",
    "#     if there is only one worker-node it will return as it is\n",
    "    if len(scalable_wns) < 2:\n",
    "        return scalable_wns\n",
    "\n",
    "#     creating a new list so that we can modify the original one\n",
    "    for wn in list(scalable_wns):\n",
    "        if wn.load <= MIN_WN_LOAD:\n",
    "            min_load_wns.append(wn)\n",
    "            scalable_wns.remove(wn)\n",
    "#             all_nodes.pop(wn.name)\n",
    "    \n",
    "    first_wn = scalable_wns[0]\n",
    "#     print(first_wn)\n",
    "    for wn in min_load_wns:\n",
    "        for pod in wn.pods:\n",
    "            \n",
    "            new_pod_name = \"pod\" + str(len(wn.pods) + 1)            \n",
    "            wn.full_component[RefPath.PODS][new_pod_name] = pod.full_component\n",
    "            wn.full_component[RefPath.PODS][new_pod_name]['id'] = new_pod_name\n",
    "            \n",
    "            for container in pod.containers:\n",
    "                filtered_methods = filter(lambda method: method.ref_path.full_path == container.ref_path, methods)\n",
    "                \n",
    "                for method in filtered_methods:\n",
    "                    print(\"Changed from:\", method.ref_path.full_path)                    \n",
    "                    method.ref_path.pod_name = new_pod_name\n",
    "                    method.ref_path.worker_node = first_wn.name\n",
    "                    print(\"To:\", method.ref_path.full_path)\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metrics': {'load': 20}, 'name': 'wn2', 'pods': {'pod1': {'name': 'pod1', 'metrics': {'load': 51}, 'containers': {'c1': {'id': 'c1', 'metrics': {'load': 30}}, 'c2': {'id': 'c2', 'metrics': {'load': 27}}, 'c3': {'id': 'c3', 'metrics': {'load': 16}}, 'c4': {'id': 'c4', 'metrics': {'load': 13}}}}}}\n",
      "no need to scale  wn1 ---- 40 (#/info/x-clusters/cl1/worker-nodes/wn1) New ? False\n",
      "pod1 ---- 51 (#/info/x-clusters/cl1/worker-nodes/wn1/pods/pod1) New ? False  need scaling.\n",
      "pod2 ---- 80 (#/info/x-clusters/cl1/worker-nodes/wn1/pods/pod2) New ? False  need scaling.\n",
      "pod3 ---- 30 (#/info/x-clusters/cl1/worker-nodes/wn1/pods/pod3) New ? False  need scaling.\n",
      "wn2 ---- 20 (#/info/x-clusters/cl1/worker-nodes/wn2) New ? False  need scaling.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "clusters, methods = _derive_components(copy.deepcopy(dataset[0]))\n",
    "scalable_wns, scalable_pods = _get_scalable_components(clusters, methods)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
